# Networks and layers; network and layer operators.

using LinearAlgebra

mutable struct ğ“› 
    
    neurons::Vector 
    W::Matrix
    biases::Vector

    function ğ“›(neurons::Vector, W::Matrix, biases::Vector)
        new(neurons, W, biases)
    end
end

mutable struct ğ“
    net::Dict{Int, ğ“›}
    dims::Vector
    nlayers::Int
    nparams::Int

    function ğ“(dims::Vector)
        structure = Dict()
        structure[1] = ğ“›(zeros(Float32, dims[1]), Array{Float32}(undef, 0, 0), [])
        for i in 2:length(dims)
            neurons = zeros(Float32, dims[i])
            weights = rand(Float32, dims[i], dims[i-1])
            biases = rand(Float32, dims[i])
            structure[i] = ğ“›(neurons, weights, biases)
         end
        n::Int32 = 0
        for i in 2:length(dims)
            n += dims[i - 1] * dims[i] + dims[i]
        end

        new(structure, dims, length(dims), n)
    end 
end

function âŠ•(Lâ‚::ğ“›, Lâ‚‚::ğ“›)
    """Addition operator under over the set ğ‘³ of layer objects.
    Observe that (ğ‘³, âŠ•) is a non-abbelian group. In particular,
    the operation

                    â„’â‚ âŠ• â„’â‚‚ = â„’â‚ƒ,          â„’áµ¢âˆˆ ğ‘³

    is neuron-preserving with respect to â„’â‚. This is, 
    â„’â‚ƒ has the same activations as â„’â‚"""

    Lâ‚ƒ = ğ“›(Lâ‚.neurons, Lâ‚.W + Lâ‚‚.W, Lâ‚.biases + Lâ‚‚.biases)
    return Lâ‚ƒ
end

function âŠ—(Î»::Number, L::ğ“›)
    """Scalar-layer multiplication.
    âŠ— is the group action

                âŠ— : â„ Ã— ğ‘³ â†’ ğ‘³ 
    """

    W = Î» * L.W
    b = Î» * L.biases
    return ğ“›(L.neurons, W, b)
end

function âŠ—(Î»::Number, N::ğ“)
    """Scalar-network multiplication âŠ—.
    Might be thought of as a group action 

                âŠ— : â„ Ã— ğ‘µ â†’ ğ‘µ

    This operation is replacing. """

    new = ğ“(N.dims)
    keys = [1:N.nlayers;]
    layers = [âŠ—(Î», N.net[i]) for i in 1:N.nlayers]
    new.net = Dict(keys .=> layers)
    return new
end

function âŠ•(Nâ‚::ğ“, Nâ‚‚::ğ“)
    """Addition operator over the set ğ‘µ of layer objects.
    Observe that (ğ‘µ, âŠ•) is a non-abbelian group. In particular, 

                    ğ§â‚ âŠ• ğ§â‚‚ = ğ§â‚ƒ           ğ§áµ¢âˆˆ ğ‘µ

    is neuron-preserving with respect to ğ§â‚."""

    if Nâ‚.dims != Nâ‚‚.dims
        throw(DimensionMismatch)
    end
    Nâ‚ƒ = ğ“(Nâ‚.dims)
    Nâ‚ƒ.net = mergewith(âŠ•, Nâ‚.net, Nâ‚‚.net)
    return Nâ‚ƒ
end

#function gradient_norm(â–¿net::ğ“)
#    #Modificarla para que concuerde con las funciones de Santi
#    norm = 0 #Float32
#    for l in 1:(â–¿net.nlayers-1)
#        norm += norm(network.net[l].weights)^2 + norm(network.net[l].biases)^2
#    end
#    sqrt(norm)
#end

