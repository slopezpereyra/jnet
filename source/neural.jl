nclude("math.jl")
include("neural_structure.jl")
using LinearAlgebra
using MLDatasets: MNIST
using Random
using Statistics
using Serialization
using Shuffle

function compute_layer(W::Matrix, a::Vector, b::Vector)
    Y = zeros(Float32, length(b))
    mul!(Y, W, a)
    Y = Y + b
    return Y
end

function fprop(x::Vector, N::ğ“, fâ‚•=Ïƒ, fâ‚’=Ïƒ)
    N.net[1].neurons = x
    L = N.nlayers
    for i in 1:(L-2)
        W = N.net[i+1].W
        a = N.net[i].neurons
        b = N.net[i+1].biases
        z = compute_layer(W, a, b)
        N.net[i+1].neurons = fâ‚•(z)
    end
    # Output layer with special function
    W = N.net[L].W
    a = N.net[L-1].neurons
    b = N.net[L].biases
    z = compute_layer(W, a, b)
    N.net[L].neurons = fâ‚’(z)
    return(N)
end

function backprop(N::ğ“, yâƒ—::Vector)
    âˆ‡ğ“ = ğ“(N.dims)
    âˆ‚Câˆ‚aá´¸ = 2 * (N.net[N.nlayers].neurons - yâƒ—)
    for l in reverse(2:N.nlayers)
        âˆ‚Ïƒâˆ‚z = dÏƒdx(logit.(N.net[l].neurons))
        P = hadamard(âˆ‚Câˆ‚aá´¸, âˆ‚Ïƒâˆ‚z)
        âˆ‡W = kron(P, transpose(N.net[l-1].neurons))
        âˆ‡ğ“.net[l].W = âˆ‡W
        âˆ‡ğ“.net[l].biases = P
        # Change the dimension of the vector appropriately
        âˆ‚Câˆ‚aá´¸ = zeros(Float32, size(N.net[l].W, 2))
        mul!(âˆ‚Câˆ‚aá´¸, transpose(N.net[l].W), P)
    end
    return âˆ‡ğ“
end

function trainset_gradient(input_data::MNIST, network::ğ“,
    hidden_activation::Function=Ïƒ,
    output_activation::Function=Ïƒ)
    """
    Computes the average gradient for the whole training set
    for a given network.
    """
    costs = 0
    âˆ‡N = ğ“(network.dims)
    # Substract network parameters by themselves, so that all parameters are zero.
    minusâˆ‡N = âŠ—(-1, âˆ‡N)
    âˆ‡N = âŠ•(âˆ‡N, minusâˆ‡N)
    nsamples = length(input_data.targets)
    @time for i in 1:nsamples
        input = vec(input_data[i].features)
        network = fprop(input, network,
                                hidden_activation, output_activation)
        # Adds all weights and biases of â–¿net and the gradient of the cost
        # in the network so as to compute the average at the end.s
        costs += cost(network.net[network.nlayers].neurons, input_data[i].targets)
        y = get_target_vector(input_data[i].targets)
        gradient = backprop(network, y)
        âˆ‡N = âŠ•(âˆ‡N, gradient)
    end
    # Average gradient over all training set:
    return (âŠ—(1 / nsamples, âˆ‡N), costs/nsamples)
end

function train(input_data::MNIST, network::ğ“, convergence_criteria::Number, Î·)
    if Î· <= 0
        error("Cannot train with non-positive learning rate")
    end
    cost = 1000
    costs = []
    epoch = 1
    while cost > convergence_criteria
        print("Starting epoch ", string(epoch), ". Cost: ", cost)
        avg_gradient = trainset_gradient(input_data, network) 
        network = âŠ•(network, âŠ—(-Î·, avg_gradient[1]))
        cost = avg_gradient[2]
        append!(costs, cost)
        epoch += 1

    end
    return (network, costs)
end

function save_network(network::ğ“, filename="net"::String)
    open(filename * ".bin", "w") do f
        serialize(f, network)
    end
end

function load_network(filename)
    net = deserialize(filename)
    return net
end

function get_mini_batches(data::Vector, size::Int, n::Int)
    shuffled = shuffle(data)
    batches = [shuffled[1:size]]
    for i in 2:n
        append!(batches, [shuffled[(i*size+1):(i+1)*size]])
    end
    return batches
end

function get_target_vector(target::Int)::Vector
    target_vector = zeros(Float32, 10)
    target_vector[target+1] = 1
    return target_vector
end
